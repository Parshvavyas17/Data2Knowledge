{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parshvavyas17/Data2Knowledge/blob/main/Data2Knowledge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxL75kINUUKg",
        "outputId": "2b424b60-84dd-436e-9faa-97cf6d803133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.8/dist-packages (3.0.1)\n",
            "Requirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from PyPDF2) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyMuPDF==1.16.14 in /usr/local/lib/python3.8/dist-packages (1.16.14)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.8/dist-packages (0.26.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai) (3.8.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.8/dist-packages (3.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.8/dist-packages (0.8.0)\n",
            "Requirement already satisfied: Wand>=0.6.10 in /usr/local/lib/python3.8/dist-packages (from pdfplumber) (0.6.11)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.8/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pdfminer.six==20221105 in /usr/local/lib/python3.8/dist-packages (from pdfplumber) (20221105)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.8/dist-packages (from pdfminer.six==20221105->pdfplumber) (39.0.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from pdfminer.six==20221105->pdfplumber) (2.1.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install PyMuPDF==1.16.14\n",
        "!pip install openai\n",
        "!pip install wget\n",
        "!pip install pdfplumber\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "import openai\n",
        "import wget\n",
        "import pathlib\n",
        "import pdfplumber\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "GzmXMtxQHT3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'Research Paper 1.pdf'\n",
        "reader = PdfReader(filename)\n",
        "text=\"\"\n",
        "for i in range(0, len(reader.pages)):\n",
        "  text+=reader.pages[i].extract_text()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2PUKSgHHWg-",
        "outputId": "4003be28-574b-42df-ab74-4de82279bd44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Research Article\n",
            "Prospects and Challenges of Using Machine Learning for\n",
            "Academic Forecasting\n",
            "Edeh Michael Onyema ,1Khalid K. Almuzaini ,2Fergus Uchenna Onu,3\n",
            "Devvret Verma ,4Ugboaja Samuel Gregory,5Monika Puttaramaiah ,6\n",
            "and Rockson Kwasi Afriyie7\n",
            "1Department of Mathematics and Computer Science, Coal City University, Enugu, Nigeria\n",
            "2National Center for Cybersecurity Technologies (C4C), King Abdulaziz City for Science and Technology (KACST),\n",
            "Riyadh 11442, Saudi Arabia\n",
            "3Department of Computer Science, Ebonyi State University, Abakiliki, Nigeria\n",
            "4Department of Biotechnology, Graphic Era Deemed to be University, Dehradun, Uttarakhand, India\n",
            "5Department of Computer Science, Michael Okpara University of Agriculture, Umuahia, Nigeria\n",
            "6Department of Machine Learning, BMS College of Engineering, Bengaluru, India\n",
            "7Department of Information and Communication Technology, Dr. Hilla Limann Technical University, WA, Ghana\n",
            "Correspondence should be addressed to Rockson Kwasi Afriyie; krafriyie@st.knust.edu.gh\n",
            "Received 10 February 2022; Revised 19 April 2022; Accepted 27 April 2022; Published 17 June 2022\n",
            "Academic Editor: Ziya UddinCopyright ©2022 Edeh Michael Onyema et al. /T_his is an open access article distributed under the Creative Commons Attribution\n",
            "License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is\n",
            "properly cited.\n",
            "/T_he study examines the prospects and challenges of machine learning (ML) applications in academic forecasting. Predicting\n",
            "academic activities through machine learning algorithms presents an enhanced means to accurately forecast academic events,\n",
            "including the academic performances and the learning style of students. /T_he use of machine learning algorithms such as K-nearest\n",
            "neighbor (KNN), random forest, bagging, arti/f_icial neural network (ANN), and Bayesian neural network (BNN) has potentials\n",
            "that are currently being applied in the education sector to predict future events. Many gaps in the traditional forecasting\n",
            "techniques have greatly been bridged by the use of arti/f_icial intelligence-based machine learning algorithms thereby aiding timely\n",
            "decision-making by education stakeholders. ML algorithms are deployed by educational institutions to predict students’ learning\n",
            "behaviours and academic achievements, thereby giving them the opportunity to detect at-risk students early and then develop\n",
            "strategies to help them overcome their weaknesses. However, despite the bene/f_its associated with the ML approach, there exist\n",
            "some limitations that could aﬀect its correctness or deployment in forecasting academic events, e.g., proneness to errors, data\n",
            "acquisition, and time-consuming issues. Nonetheless, we suggest that machine learning remains one of the promising forecasting\n",
            "technologies with the power to enhance eﬀective academic forecasting that would assist the education industry in planning and\n",
            "making better decisions to enrich the quality of education.\n",
            "1. Introduction\n",
            "Machine learning is rapidly gaining in/f_luence in the educa-\n",
            "tion industry and can possibly strengthen many importantareas of teaching and learning, research, and decision-making[1]. It is about mimicking humans in terms of both reasoning\n",
            "and behaviour [2, 3]. Na ¨ıve Bayes, decision tree, and manyother related algorithms have prognostic capabilities that are\n",
            "of great interest to medical educators and [4] and are beingused to facilitate learning [5]. Machine learning models learnfrom experiences or observations and then become veryactive to achieve human-like actions [6, 7]. Investigationshave shown that ML has forecasting capabilities by gaining an\n",
            "understanding of each student’s strengths and weaknesses, asHindawi\n",
            "Computational Intelligence and Neuroscience\n",
            "Volume 2022, Article ID 5624475, 7 pages\n",
            "https://doi.org/10.1155/2022/5624475well as reasons for why they may be struggling in the\n",
            "classroom [8, 9].\n",
            "Forecasting of learning outcomes using the machine\n",
            "learning approach can help educational institutions tounderstand the learning patterns and behaviours of theirstudents, which can be useful in improving school pol-\n",
            "icies, curriculum preparations, and teacher methodology\n",
            "[10]. Indeed, machine learning algorithms oﬀer oppor-tunities for organizations, including educational insti-tutionstoachieveminimalforecasterrors(accuracy)andimprove their decision-making process and operationalperformances [11]. ML provides improvements that helpreduceforecastmistakesorerrorsthathavebeenasourceofconcernstomanyforecastersandtheirclientsinrecenttimes. (Y_he inaccuracy in forecasting often causes majorproblems to systems and could lead to poor decision-making and system failure. With machine learning\n",
            "models that monitor learners’ progress and recommend\n",
            "next steps have been achieved, and learning analytics arealso made possible to accommodate special learners andthose that require more attention or personalizedteaching. (Y_his study reviews the beneﬁts of ML and itsshortcomings when deployed for academic forecasting.(Y_he study would go a long way to assist forecasters andeducation authorities in understanding the emergingprospects and challenges of machine learning and howbest to apply it in the education sector to minimizestudent’s failures and to enhance the decision-makingprocess. (Y_his investigation adds to the body of infor-\n",
            "mation about the educational applications of machine\n",
            "learning and artiﬁcial intelligence.\n",
            "2.Related Literature\n",
            "Several researchers [7, 10, 12–19] have highlighted thegrowingapplicationandusefulnessofmachinelearningasatool for forecasting of events in the academic sector. Sim-ilarly,astudy by Elhaj etal. [13] provedthe exactnessof the\n",
            "KNN algorithm in detecting the student’s learning style,\n",
            "which is fast becoming helpful in training at-risk students.\n",
            "Halde [20] gave analysis of various ways in which ma-\n",
            "chine learning can be applied by educational institutions topredictstudents’performanceandthecriticalcharacteristicsthat have to be considered while making such predictions.(Y_he researcher discovered that ML enhances more preciseand accurate prediction of academic performances, butfeaturessuch aslearningstyles,motivation,and manyotherfactorsrelatingtostudents’learninghavetobeconsideredtoachieve accuracy. Essinger and Rosen [21] presented a\n",
            "system to aid students’ motivation and interests in problem\n",
            "solving using machine learning.\n",
            "Dhanabal and Chandramathi [22] and Itoh et al. [23]\n",
            "developed a model based on the machine learning al-gorithmwiththecapacitytoaccuratelyforecaststudents’academic outputs based on their historical or previousrecord of performances. (Y_hey achieved success in thestudy as students’ academic standings were predicted,thereby enabling those at risk to receive some help thatenhanced their performances. Research on ML is fastgrowing, but there are gaps in the literature regarding itsprospects and challenges in academic forecasting in re-cent times as most existing studies seemed to have beenconducted years ago. (Y_hus, this study provides updatedknowledge in this regard. (Y_he summary of the review isillustrated in Table 1.\n",
            "3.Overview of Machine Learning\n",
            "Machine learning is about training machines to learncertain behaviours or traits and then think and make de-cisions based on the learned behaviour. It remains one ofthe promising areas of research to solve many societalproblems and can be classiﬁed as supervised and unsu-pervised learning techniques. (Y_hrough the use of an un-supervisedmethod,youmayseeifasystemcanderivedata\n",
            "and inferences even when there are not any results or\n",
            "training data. While in the supervised learning ﬁeld, foreach observation of the predictor x\n",
            "i,i�1,. . .,nthere is a\n",
            "related outcome yi[7]. Supervised algorithms facilitate the\n",
            "understandingofthelearninghistoryofthesystemandthecorrectness of the output for future analysis. Machinelearning can be applied in diﬀerent areas, including DNAsequence classiﬁcation, image processing such as face de-tection, speech recognition aka natural language process-ing, security, search engine algorithms, and academicforecasting [34, 35].\n",
            "Accordingtotrueinteraction.com[36]andEdehetal.\n",
            "[37], predictions based on machine learning can be re-liable if the dataset is properly trained and the validationof the model is right. Indeed, the prospects of ML arehuge, and as the education industry embraces new re-alities, ML together with other emerging technologiescould play a bigger role in learning analytics, e-learning,students’ examinations and performance prediction andtracking of teaching, and learning progress. Severalmachine learning algorithms such as multilayer per-ceptron (MLP) and CARTregression trees (CART) havediﬀerent potentials which are of great interest to stake-\n",
            "holders in the academic sector. For machine learning to\n",
            "be able to understand a dataset, it has to be trained andalsopassthroughdiﬀerentprocessestofullycomprehendsuchdataandbeabletointerpretandenhancepredictionor any other expected decisions. (Y_his study looks at thepros and cons associated with machine learning use inacademic forecasting compared to other forecastingmethods. Consequently, Table 2 highlights the compar-isonbetweenthemachinelearning-basedforecastingandthe traditional forecasting technique.\n",
            "(Y_he comparison between traditional and machine\n",
            "learning forecasting techniques is further explained inFigures 1 and 2 adopted from the Genpact.com website.\n",
            "Figure 1 shows the stages applied in the traditional\n",
            "forecasting technique. While Figure 2 depicts diﬀerentstages that forecasting through machine learning goesthrough before generating an output. As it can be seen,Figure 2 contains more robust or rigorous procedures thatactually improve the accuracy of the output that is2 ComputationalIntelligenceandNeuroscienceproduced compared to that of the traditional method\n",
            "shown in Figure 1.\n",
            "4.Prospects of Forecasting Academic\n",
            "Events with Machine Learning\n",
            "(Y_heevolutionofartiﬁcialintelligencehasincreasedtheuseof machine learning as a means to achieve academicprediction precision and timeliness [42]. Many educa-tional institutions are utilising machine learning algo-rithmsandtheiraccompanyingfeaturestopredictpossiblefuture events and outcomes to aid decision-makers inmakingthebestpossibledecisions.MLhasbecomeagoodalternative to traditional statistical methods of forecasting[39]. Some of the beneﬁts of forecasting with ML areexplained in the following section.\n",
            "4.1. Pattern Recognition. Machine learning models can be\n",
            "trained with large volumes of data which it tends to un-\n",
            "derstandlaterandﬁgureoutdiﬀerenttrendsandpatternsinthe data that ordinarily would prove very diﬃcult forhumans to comprehend. Decision-makers can examinetrends and seasonal realities that aﬀect their organization’sperformancebyunderstandingandanalyzingthesepatterns[43]. In academics, learners’ pattern is closely monitoredtherebyenablingeducatorstoteachbetterandalsodiagnoselearning diﬃculties of students with a view to assist vul-nerable students.Table1: Summary of related works.\n",
            "Authors Outcome\n",
            "Musso et al. [10](Y_he study successfully forecasted students’ academic success one year ahead using the ANN based on cognitive\n",
            "and demographic traits\n",
            "Hudson and Cristiano\n",
            "[7](Y_he results suggest that ML can generate dependable results in prediction\n",
            "Elhaj et al. [13] (Y_he study was empirical, and it showed the ability of KNN in prediction of learning patterns of students\n",
            "Ahajjam et al. [24](Y_hepaperprovidedAI-basedsolutionstotrackstudents’performanceandwasabletorecommenddiagnosisfor\n",
            "the Moroccan students\n",
            "Pranav et al. [25] (Y_he paper provided evidence on the signiﬁcance of AI in management of education data and decision-makingLidia et al. [26](Y_he paper concluded that ML will be required more in the future because of the need to assist students to\n",
            "overcome learning diﬃculties and also enhance their productivity in learning\n",
            "Phauk and Takeo [27](Y_he study recommended the use of the hybrid machine learning algorithm approach to solve misclassiﬁcation\n",
            "issues and improve academic prediction accuracy\n",
            "Onan and Koruko˘ glu\n",
            "[28](Y_he research proposed an ensemble method to feature selection that combines the results of numerous\n",
            "independent feature lists generated by various features that may be used in education\n",
            "Onan [29] (Y_he study provided a better approach for managing students’ information system via ML\n",
            "Hassen et al. [30](Y_he study showed that the student’s success with the aid of machine learning can be monitored using their\n",
            "previous performance data before they engaged in the current program\n",
            "Ibtehal [31] (Y_he study aﬃrmed the applicability of ML in education technology development and deployment\n",
            "FedersandAnders[32](Y_hey developed a smart algorithm that assessed the teaching methods of teachers and how it aﬀects the\n",
            "understandingoftheirlessonsbystudentsintheclasstakingintoconsiderationtheformerknowledgeofstudents\n",
            "Popenici and Kerr [33] (Y_hey examined the various implications of ML and other relevant AI-driven systems in higher education\n",
            "Table2: Machine learning-based forecasting vs. traditional forecasting technique.\n",
            "Machine learning forecasting Traditional forecasting\n",
            "It gives more accurate predictions with minimal loss function [10, 13] Forecast errors are more likely to occur [38]\n",
            "(Y_he approach is more scientiﬁcally driven [26]Suﬀers a lot from assumptions leading to subjective\n",
            "conclusions at times [7]\n",
            "Very demanding in computation [39] Less demanding computation\n",
            "It is more prone to underﬁtting and overﬁtting issues [40] Less prone to underﬁtting and overﬁtting issues\n",
            "Focuses more on result or outcome, but silent relationships among variables. Relationship between variables are often highlightedHighlyrecommendedinapplicationswherethegoalistolearnfromdatasetswith\n",
            "a large number of characteristics [41]Suitable in univariate applications often meant to\n",
            "assess and summarize data.\n",
            "It can work with massive data It works with limited or historical data\n",
            "Historical \n",
            "Data InputData Pre-\n",
            "processingStatistical \n",
            "TestingTraditional \n",
            "Forecasting \n",
            "AlgorithmsStatistical \n",
            "Forecast \n",
            "Output\n",
            "Figure1: (Y_he traditional forecasting approach [41].ComputationalIntelligenceandNeuroscience 34.2. Reduced Human Intervention. Since machine learning\n",
            "has the ability to learn and interpret the learnt dataset,\n",
            "forecasters often do not need to interfere much with theprediction process. (Y_his is because the machine learningalgorithms can automatically make predictions using thepattern of data in the set. (Y_his act minimizes human bias or\n",
            "interferences experienced in the traditional forecasting ap-\n",
            "proach[44].Asitcanbeseeninthefunctioningofantivirussoftware, they do not require human intervention to rec-ognize potential attacks or threats in the computer beforedetecting or blocking them. It does that autonomously andautomatically.\n",
            "4.3. Accurate Academic Forecasting. Machine learning al-\n",
            "gorithms gain, learn, and acquire experiences overtime,\n",
            "which help them become better and better in their pre-diction ability. (Y_he use of the ML forecasting method can\n",
            "improvetheaccuracyandeﬃciencyofacademicpredictions\n",
            "comparedtootherpredictiontechniquessuchastimeseries[39]. It makes it easier to generate data to identify at-riskstudentsandthendevelopdiagnosticmeasurestohelpthemovercometheirweaknesses[45].Forecastingaccuracyisvitalas it helps educational institutions to make adequateplanning and preparations towards future events whichinclude risks, thereby giving them enough time to ﬁndmeasures to mitigate its eﬀects. However, machine learningalgorithmsdependonseveralelements,includingthequalityand speed with which the machine learns and improves its\n",
            "performance, which might aﬀect its precision. Predictive\n",
            "outcomes may be aﬀected by changes in a model variable[46].\n",
            "4.4. Forecasting of Large Volume of Education Data. In this\n",
            "digital era with high demand for a wide range of education\n",
            "data, machine learning is the right method to analyze orinterpret and deal with a broad range of data and can alsorespond to changes in data and the environment. Largedatasets can easily be mined and meaningful patterns pre-\n",
            "dicted with minimum use of resources [47].\n",
            "4.5. Continuous Improvement. (Y_he ability of machine\n",
            "learning to continuously learn and adapt to changing sit-\n",
            "uations presents advantages in the academic sector. (Y_his isbecause the education sector is dynamic, and changes arealways expected most times. (Y_he ML algorithm is able toadjust quickly and accommodate these emerging changesand improve itself rapidly [48]. Whenever it comes toproducing predictions, machine learning is a better option\n",
            "because of its speed.\n",
            "4.6. Academic Cost Reduction. Machine learning generally\n",
            "relies on very strong assumptions about the statistical sta-\n",
            "bility of the environment. (Y_he prediction of expected futurehappenings would enable educational institutions to plantheir admissions, policies, and priorities thereby enhancingtheir preparedness and reducing cost [49]. Accurate pre-dictions via machine learning go a long way to assist policymakers and other education stakeholders to realign theirfocus in line with trends and also save cost that could haveemanated from poor decision-making as a result of lack ofaccurate prediction.\n",
            "4.7. Management of Unexpected Closure of Schools.\n",
            "Machine learning can be relied on by education institutions\n",
            "tohelptheminterpretdatarelatingtoemergenciesespecially\n",
            "as it concerns crisis that can lead to shutdown of schools.During the recent COVID-19 crisis, machine learningplayed an important role in interpreting data that wererelevant in reopening of schools and development ofstrategies for post-COVID-activities.\n",
            "(Y_he various prospects and beneﬁts of machine learning\n",
            "in academic forecasting and other areas in education aresummarized in Figure 3.\n",
            "5.Challenges of Forecasting with\n",
            "Machine Learning\n",
            "(Y_here are ﬂaws in machine learning. Similar to other ma-chine and human forecasting methods, machine learninghas its own set of ﬂaws. However, it might be claimed that\n",
            "machinelearning presentssigniﬁcantlyfewer obstaclesthan\n",
            "traditional forecasting methods [48]. For example, machinelearning does not totally depend on historical data to makepredictions, it learns from experiences and understandspatterns, which may not have existed earlier but are learntduring the training of the model. However, machinelearning has many limitations in performance evaluation[49]. Some of the challenges associated with the use of MLfor forecasting are categorized as follows:\n",
            "5.1. Proneness to Errors. Evidence has shown that ML is\n",
            "autonomous but highly susceptible to both machine and\n",
            "human errors [13]. For instance, if the right amount of data\n",
            "isnottrained inthedataset,biasedpredictionscanbe madeandgeneralized[50].(Y_hisisbecausethequalityofpredictionwould largely depend on the correctness of the trainingdataset.PredictionerrorsinMLoftenappearwhichareverydiﬃcult to diagnose and correct because they require rig-orous underlying complexities of the algorithms and asso-ciated processes [43].\n",
            "5.2. Data Acquisition. ML algorithms rely on data to learn\n",
            "andpredictacademicevents.Itisdatahungry.However,the\n",
            "acquisition of these data is not easy, but the larger the data,\n",
            "the better the machine learning and prediction reliability\n",
            "[43].\n",
            "5.3. Time Factors. Machine learning algorithms require\n",
            "suﬃcient time to train and learn to be able to function\n",
            "eﬀectively. (Y_his could lead to time wastage. (Y_he ML fore-casting technique, on the other hand, necessitates patienceand time to assure accuracy. (Y_he time complexity couldincrease depending on the size of data. (Y_he more the dataare, the more time would be required.4 ComputationalIntelligenceandNeuroscience5.4. Veriﬁcation Challenges. Sometimes, it is diﬃcult to\n",
            "verify certain facts that are not included in the historical\n",
            "data, which means that ML predictions may not be ac-curateincertaincases.Itcouldalsotakesometimeforthe\n",
            "model to be trained or master the dataset for eﬀective\n",
            "decision.\n",
            "6.Conclusion\n",
            "Machine learning presents solutions to many limitations inthe education sector, including academic forecasting. Notonly does it oﬀer intelligent and accurate academic pre-dictions, but it also assists the teachers and educationalinstitutions to understand their students’ betterment andhelp them succeed. (Y_hough, many educational institutionsin developing countries seem not ready to adopt machinelearning approaches, but the changing realities in global\n",
            "educationwouldleavethemwithnochoicethantobeginto\n",
            "strategize on how to adopt ML and other emerging tech-nologies. Prediction through machine learning and otherartiﬁcial intelligence solutions has the potential to changethe narratives in the education sector, particularly the di-agnosing of students’ learning patterns and performance.Our future projects will focus on development of the ANNmodel for academic outcome prediction in secondaryschools in Enugu Nigeria.\n",
            "Data acquisition \n",
            "and definition of \n",
            "ObjectiveData \n",
            "Understanding and \n",
            "cleanupData preprocessing \n",
            "feature engineering \n",
            "before training ML \n",
            "model\n",
            "Training ML model Building ML ModelSplitting the \n",
            "dataset into \n",
            "training and \n",
            "testing segments\n",
            "Evaluating model \n",
            "effectiveness by \n",
            "comparing forecast \n",
            "with actual valuesComparing \n",
            "performance of \n",
            "various modelsSelecting the model \n",
            "with least weighed \n",
            "mean absolute \n",
            "percentage errorFigure2: Machine learning forecasting approach [41].\n",
            "Reduced human \n",
            "interventionForecast large \n",
            "volume of data\n",
            "Learner progress \n",
            "trackingIntelligent \n",
            "tutoring systemSchool \n",
            "automation\n",
            "Understanding \n",
            "of students’ \n",
            "needsAccuracy\n",
            "Improve response \n",
            "in emergency Data\n",
            "interpretation\n",
            "/Advanced\n",
            "analytics\n",
            "Automatic \n",
            "gradingBenefits of \n",
            "Machine \n",
            "Learning in \n",
            "Academic \n",
            "Settings\n",
            "Improve \n",
            "learner \n",
            "experienceFuture prediction \n",
            "and \n",
            "planning\n",
            "Customized \n",
            "education\n",
            "Figure3: Beneﬁts of machine learning in academic setting and forecasting.ComputationalIntelligenceandNeuroscience 5Data Availability\n",
            "All data have been cited within the paper.\n",
            "Conflicts of Interest\n",
            "(Y_he authors declare that there are no conﬂicts of interest.\n",
            "Acknowledgments\n",
            "(Y_he authors thank the King Abdulaziz City for Science and\n",
            "Technology (KACST) for its support partially in thisresearch.\n",
            "References\n",
            "[1] A. J. Nair, H. Hajin, and M. D. Norazryana, “Machines\n",
            "learning trends, perspectives and prospects in education\n",
            "sector,” in Proceedings of the 2019 3rd International Confer-\n",
            "ence on Education and Multimedia Technology, pp. 201–205,Nagoya, Japan, July 2019.\n",
            "[2] K. Krishnan, “Data-driven architecture for big data,” Data\n",
            "Warehousing in the Age of Big Data-MK Series on Business\n",
            "Intelligence, pp. 219–240, Morgan Kaufmann, MA, USA,\n",
            "2013.\n",
            "[3] V. Roy, S. Shukla, P. K. Shukla, and P. Rawat, “Gaussian\n",
            "elimination-based novel canonical correlation analysis\n",
            "method for EEG motion artifact removal,” Journal of\n",
            "Healthcare Engineering, vol. 2017, Article ID 9674712,11 pages, 2017.\n",
            "[4] E. M. Onyema, T. A. Ahanger, G. Samir et al., “Empirical\n",
            "analysis of apnea syndrome using an artiﬁcial intelligence-\n",
            "based granger panel model approach,” Computational Intel-\n",
            "ligence and Neuroscience, vol. 2022, Article ID 7969389,7 pages, 2022.\n",
            "[5] M. Schneider and P. Edelsbrunner, “Modeling for prediction\n",
            "vs.Modelingforunderstanding,” FrontlineLearningResearch,\n",
            "vol. 1, no. 2, pp. 99–101, 2013.\n",
            "[6] P.Flach, MachineLearning:LT_heArtandScienceofAlgorithms\n",
            "that Make Sense of Data, Cambridge University Press,Cambridge, UK, 2012.\n",
            "[7] F.G.HudsonandM.A.G.Cristiano,“Fourmachinelearning\n",
            "methodstopredictacademicachievementofcollegestudents:a comparison study. Reviista eletr ´oniica de psiicologiia,”\n",
            "EDUCAÇÃO E SA ´UDE, vol. 1, no. 4, pp. 68–101, 2014.\n",
            "[8] N. Anozie and B. W. Junker, “Predicting end-of-year ac-\n",
            "countability assessment scores from monthly student recordsin an online tutoring system,” in Proceedings of the Educa-\n",
            "tional Data Mining: Papers from the AAAI Workshop, MA,USA, July 2006.\n",
            "[9] M. O. Edeh, O. I. Khalaf, C. A. Tavera et al., “A classiﬁcation\n",
            "algorithm-basedhybriddiabetes predictionmodel,” Frontiers\n",
            "in Public Health, vol. 10, Article ID 829519, 2022.\n",
            "[10] M. F. Musso, E. Kyndt, E. C. Cascallar, and F. Dochy,\n",
            "“Predictinggeneralacademicperformanceandidentifyingthediﬀerential contribution of participating variables using ar-\n",
            "tiﬁcial neural networks,” Frontline Learning Research, vol. 1,\n",
            "pp. 42–71, 2013.\n",
            "[11] D.PamelaandK.Matteo,“(Y_heroleoftheforecastingprocess\n",
            "in improving forecast accuracy and operational perfor-mance,” International Journal of Production Economics,\n",
            "vol. 131, pp. 204–214, 2004.\n",
            "[12] L. Breiman, “Random forests,” Machine Learning, vol. 45,\n",
            "no. 1, pp. 5–32, 2001.[13] M. A. E. Elhaj, S. G. Bashir, I. Abdullahi, E. M. Onyema,\n",
            "A. A. Hauwa, and A. S. Hayatu, “Evaluation of the perfor-\n",
            "mance of K-nearest neighbor algorithm in determining stu-dent learning styles,” Int. J. of Innovative Sci., Eng. & Techn,\n",
            "vol. 7, no. 1, pp. 91–102, 2020.\n",
            "[14] D. Kucak, V. Juricic, and G. Dambic, “Machine learning in\n",
            "education - a survey of current research trends,” in Pro-\n",
            "ceedings of the 29th International DAAAM Symposium 2018,\n",
            "pp. 0406–0410, Zadar, Croatia, October 2018.\n",
            "[15] D.Jorge-Martinez,S.A.Butt,E.M.Onyemaetal.,“Artiﬁ-cial\n",
            "intelligence-based kubernetes container for scheduling nodes\n",
            "of energy composition,” Int. J. of System Assurance Engi-\n",
            "neering and Management, 2021.\n",
            "[16] V. Roy, P. K. Shukla, A. K. Gupta, V. Goel, P. K. Shukla, and\n",
            "S. Shukla, “Taxonomy on EEG artifacts removal methods,\n",
            "issues,andhealthcareapplications,” JournalofOrganizational\n",
            "and End User Computing, vol. 33, no. 1, pp. 19–46, 2021.\n",
            "[17] S. Stalin, V. Roy, P. K. Shukla et al., “A machine learning-\n",
            "based big EEG data artifact detection and wavelet-based re-moval: an empirical approach,” Mathematical Problems in\n",
            "Engineering, vol. 2021, Article ID 2942808, 2021.\n",
            "[18] K.S.Piyush,R.Vandana,K.S.Prashantetal.,“AnAdvanced\n",
            "EEGMotionArtifactsEradicationAlgorithm,” LT_heComputer\n",
            "Journal, 2021.\n",
            "[19] K. R. Pradeep, S. M. P. Gangadharan, W. A. Hatamleh,\n",
            "H. Tarazi, P. K. Shukla, and B. Tiwari, “Improved machinelearning method for intracranial tumor detection with\n",
            "accelerated particle swarm optimization,” Journal of\n",
            "Healthcare Engineering, vol. 2022, Article ID 1128217, 2022.\n",
            "[20] R.R.Halde,“ApplicationofMachineLearningalgorithmsfor\n",
            "betterment in education system,” in Proceedings of the 2016\n",
            "International Conference on Automatic Control and DynamicOptimization Techniques (ICACDOT), pp. 1110–1114, Pune,\n",
            "India, September 2016.\n",
            "[21] S. D. Essinger and G. L. Rosen, “An introduction to machine\n",
            "learning for students in secondary education,” in Proceedings\n",
            "of the 2011 Digital Signal Processing and Signal ProcessingEducation Meeting (DSP/SPE), Sedona, Arizona, January\n",
            "2011.\n",
            "[22] S. Dhanabal and S. Chandramathi, “A review of various\n",
            "K-nearest neighbor algorithm query processing techniques,”International Journal of Computer Application, vol. 30, no. 7,\n",
            "pp. 4–10, 2011.\n",
            "[23] Y. Itoh, H. Itoh, and K. Funahashi, “Forecasting future stu-\n",
            "dents’ academic level and analyzing students’ feature using\n",
            "schooling logs,” in Proceedings of the 2015 IEEE 4th Global\n",
            "Conference on Consumer Electronics (GCCE), Osaka, Japan,October 2015.\n",
            "[24] A. Tarik, H. Aissa, and F. Yousef, “Artiﬁcial intelligence and\n",
            "machine learning to predict student performance during theCOVID-19,” Procedia Computer Science, vol. 184, pp. 835–\n",
            "840, 2021.\n",
            "[25] P. Dabhade, R. Agarwal, K. P. Alameen, A. T. Fathima,\n",
            "R. Sridharan, and G. Gopakumar, “Educational data miningforpredictingstudents’academicperformanceusingmachine\n",
            "learning algorithms,” Materials Today Proceedings, vol. 47,\n",
            "no. 15, pp. 5260–5267, 2021.\n",
            "[26] L.Sandra,F.Lumbangaol,andT.Matsuo,“Machinelearning\n",
            "algorithm to predict student’s performance: a systematic\n",
            "literaturereview,” TEMJournal,vol.10,no.4,pp.1919–1927,\n",
            "2021.\n",
            "[27] P. Sokkhey and T. Okazaki, “Hybrid machine learning al-\n",
            "gorithms for predicting academic performance,”6 ComputationalIntelligenceandNeuroscienceInternational Journal of Advanced Computer Science and\n",
            "Applications, vol. 11, no. 1, p. 10, 2020.\n",
            "[28] A. Onan and S. Koruko ˘glu, “A feature selection model based\n",
            "ongeneticrankaggregationfor textsentimentclassiﬁcation,”JournalofInformationScience,vol.43,no.1,pp.25–38,2017.\n",
            "[29] A.Onan,“(Y_heuseofdataminingforstrategicmanagement:a\n",
            "casestudyonminingassociationrulesinstudentinformationsystem,”Croatian Journal of Education: Hrvatski ˇcasopis za\n",
            "odgoj i obrazovanje, vol. 18, no. 1, pp. 41–70, 2016b.\n",
            "[30] H. Zeineddine, U. Braendle, and A. Farah, “Enhancing pre-\n",
            "diction of student success: automated machine learning ap-proach,”Computers & Electrical Engineering, vol. 89, Article\n",
            "ID 106903, 2021.\n",
            "[31] T.N.Ibtehal,“MachineLearninginEducationalTechnology,”\n",
            "Machine Learning, IntechOpen, London, UK, 2018.\n",
            "[32] F. Duzhin and A. Gustafsson, “Machine learning-based app\n",
            "for self-evaluation of teacher-speciﬁc instructional style andtools,”Education Sciences, vol. 8, no. 1, p. 7, 2018.\n",
            "[33] S. A. D. Popenici and S. Kerr, “Exploring the impact of ar-\n",
            "tiﬁcial intelligence on teaching and learning in higher edu-cation,” Research and Practice in Technology Enhanced\n",
            "Learning, vol. 12, no. 1, p. 22, 2017.\n",
            "[34] P. Chan and S. Stolfo, “Toward scalable learning with non-\n",
            "uniform class and cost distributions: a case study in creditcardfrauddetection,”in ProceedingsoftheFifteenthNational\n",
            "Conference on Artiﬁcial Intelligence, pp. 164–168, Madison,Wisconsin, July 1998.\n",
            "[35] H.Kou-YuanandC.Wen-Lung,“ANeuralNetworkMethod\n",
            "for Prediction of 2006 World Cup Football Game,” in Pro-\n",
            "ceedingsoftheInternationalJointCouncilonNeuralNetworks,pp. 259–266, Barcelona, Spain, July 2010.\n",
            "[36] M. Davison, “AI and the Classroom: Machine Learning in\n",
            "Education,” 2021, https://www.trueinteraction.com/ai-and-the-classroom-machine-learning-in-education/.\n",
            "[37] M. O. Edeh, C. E. Nwafor, A. D. Nnaji, G. A. Fyneface,\n",
            "C.P.Obiekwe,and D.Omachi,“(Y_heimpactofinquiry-basedteaching approach on computer science learning,” EBSU\n",
            "Science Journal, vol. 1, no. 1, pp. 61–70, 2020.\n",
            "[38] J. Kumari, R. Venkatesan, T. Jemima Jebaseeli, V. Abisha\n",
            "Felsit,K.SalaiSelvanayaki,andT.JeenaSarah,“Acomparisonof machine learning techniques for the prediction of thestudent’s academic performance,” Emerging Trends in Com-\n",
            "puting and Expert Technology, pp. 1052–1062, Springer,Berlin, Germany, 2020.\n",
            "[39] S.Makridakis,E.Spiliotis,andV.Assimakopoulos,“Statistical\n",
            "and Machine Learning forecasting methods: concerns andways forward,” PLoS One, vol.13, no. 3, Article ID e0194889,\n",
            "2018.\n",
            "[40] A. Field, Discovering Statistics Using R, Sage, London, UK,\n",
            "2012.\n",
            "[41] Genpact,“(Y_heevolutionofforecastingtechniques:traditional\n",
            "versus machine learning methods,” 2022, https://www.\n",
            "genpact.com/insight/technical-paper/the- evolution-of-\n",
            "forecasting-techniques-traditional-versus-machine-learning-methods.\n",
            "[42] E.M.Onyema,N.C.Eucheria,C.U.Ezeanya,P.N.Eziokwu,\n",
            "and U. E. Ani, “Impact of E- learning platforms on students’interest and academic achievement in data structure course,”Coal City University Journal of Science, vol.1, no.1, pp.1–16,\n",
            "2020.\n",
            "[43] P.Geurts,A.Irrthum,andL.Wehenkel,“Supervisedlearning\n",
            "with decision tree-based methods in computational and\n",
            "systemsbiology,” MolecularBioSystems,vol.5,no.12,p.1593,\n",
            "2009.[44] G. Zotteri and M. Kalchschmidt, “Forecasting practices:\n",
            "empirical evidence and a framework for research,” Interna-\n",
            "tional Journal of Production Economics, vol. 108, no. 1-2,\n",
            "pp. 84–99, 2007.\n",
            "[45] P. Asthana and B. Hazela, “Applications of machine learning\n",
            "in improving learning environment,” Multimedia Big Data\n",
            "Computing for IoT Applications, pp. 417–433, Springer, Sin-gapore, 2020.\n",
            "[46] Quora, “What-are-the-limitations-rules-of-what-machine-\n",
            "learning-can-of-cannot-do,” 2022, https://www.quora.com/\n",
            "What-are-the-limitations-rules-of-what-machine-learning-can-of-cannot-do.\n",
            "[47] S. B. Kotsiantis, “Use of machine learning techniques for\n",
            "educational proposes: a decision support system for fore-\n",
            "castingstudents’grades,” ArtiﬁcialIntelligenceReview,vol.37,\n",
            "no. 4, pp. 331–344, 2011.\n",
            "[48] A. Agarwal, “Highlights the advantages and disadvantages of\n",
            "machine learning language,” 2021, https://www.cisin.com/coﬀee-break/Enterprise/highlights-the-advantages-and-disad\n",
            "vantages-of-machine-learning.html.\n",
            "[49] K. Ashok, “Why are the limitation of machine learning?,”\n",
            "2018, https://www.quora.com/Why-are-the-limitation-of-\n",
            "machine-learning.\n",
            "[50] Data Flair, “Advantages and Disadvantages of Machine\n",
            "Learning Language,” 2021, https://data-ﬂair.training/blogs/\n",
            "advantages-and-disadvantages-of-machine-learning/.ComputationalIntelligenceandNeuroscience 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# url = \"https://downloads.hindawi.com/journals/cin/2022/5624475.pdf\"\n",
        "url = \"https://arxiv.org/abs/1808.04295\"\n",
        "\n",
        "def getPaper(url, filename=\"random_paper.pdf\"):\n",
        "    \"\"\"\n",
        "    Downloads a paper from it's arxiv page and returns\n",
        "    the local path to that file.\n",
        "    \"\"\"\n",
        "    downloadedPaper = wget.download(url, filename)    \n",
        "    downloadedPaperFilePath = pathlib.Path(downloadedPaper)\n",
        "\n",
        "    return downloadedPaperFilePath"
      ],
      "metadata": {
        "id": "vO7ehvtziyEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paperFilePath = \"random_paper.pdf\"\n",
        "paperContent = pdfplumber.open(paperFilePath).pages\n",
        "\n",
        "def displayPaperContent(paperContent, page_start=0, page_end=5):\n",
        "    for page in paperContent[page_start:page_end]:\n",
        "        print(page.extract_text())\n",
        "displayPaperContent(paperContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEbgcFE3HhE1",
        "outputId": "7d687771-6e5f-4ff2-e83b-bce7157b7520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Understanding training and generalization in deep\n",
            "learning by Fourier analysis\n",
            "Zhi-QinJohnXu∗\n",
            "8102\n",
            "NewYorkUniversityAbuDhabi\n",
            "AbuDhabi129188,UnitedArabEmirateszhiqinxu@nyu.edu\n",
            "voN\n",
            "Abstract\n",
            "92\n",
            "Background: It is still an open research area to theoretically understand why\n",
            "DeepNeuralNetworks(DNNs)—equippedwithmanymoreparametersthantrain-ingdataandtrainedby(stochastic)gradient-basedmethods—oftenachievere-markablylowgeneralizationerror.Contribution:WestudyDNNtrainingbyFourieranalysis.Ourtheoreticalframeworkexplains:i)DNNwith(stochastic)gradient-basedmethodsoftenendowslow-frequencycomponentsofthetargetfunctionwithahigherpriorityduringthetraining;ii)SmallinitializationleadstogoodgeneralizationabilityofDNNwhilepreservingtheDNN’sabilitytoﬁtanyfunction.TheseresultsarefurtherconﬁrmedbyexperimentsofDNNsﬁt-tingthefollowingdatasets,thatis,naturalimages,one-dimensionalfunctionsandMNISTdataset.\n",
            "]GL.sc[\n",
            "4v59240.8081:viXra\n",
            "1 Introduction\n",
            "Background Deep learning has achieved great success as in many ﬁelds (LeCunetal. (2015)).\n",
            "Recent studies have focused on understandingwhy DNNs, trained by (stochastic) gradient-based\n",
            "methods, can generalize well, that is, DNNs often ﬁt the test data well which are not used for\n",
            "traininginpractice. Counter-intuitively,althoughDNNshavemanymoreparametersthantraining\n",
            "data,theycanrarelyoverﬁtthetrainingdatainpractice.\n",
            "Several studies have focused on the local property (sharpness/ﬂatness) of loss function at min-\n",
            "ima (Hochreiter&Schmidhuber(1995)) to explorethe DNN’s generalizationability. Keskaretal.\n",
            "(2016)empiricallydemonstratedthatwithsmallbatchineachtrainingstep,DNNsconsistentlycon-vergetoﬂatminima,andleadtoabettergeneralization.However,Dinhetal.(2017)arguedthatmostnotionsofﬂatnessareproblematic.Tothisend,Dinhetal.(2017)useddeepnetworkswithrectiﬁerlinearunits(ReLUs)totheoreticallyshowthatanyminimumcanbearbitrarilysharporﬂatwithoutspecifyingparameterization.Withtheconstraintofsmallweightsinparameterization,Wuetal.(2017)provedthatfortwo-layerReLUnetworks,low-complexitysolutionslieintheareaswithsmallHessian,thatis,ﬂatandlargebasinsofattractor(Wuetal.(2017)).Theythenconcludedthatarandominitializationtendstoproducestartingparameterslocatedinthebasinofﬂatminimawithahighprobability,usinggradient-basedmethods.\n",
            "Several studies rely on the concept of stochastic approximation or uniform stability\n",
            "(Bousquet&Elisseeff (2002), Hardtetal. (2015)). To ensure the stability of a training algorithm,\n",
            "Hardtetal.(2015)assumedlossfunctionwithgoodproperties,suchasLipschitzorsmoothcondi-tions.However,thelossfunctionofaDNNisoftenverycomplicated(Zhangetal.(2016)).\n",
            "∗ThisworkisdonewhileXuisavisitingmemberatCourantInstituteofMathematicalSciences,NewYork\n",
            "University,NewYork,UnitedStates.\n",
            "Preprint.Workinprogress.\n",
            "Another approach to understanding the DNN’s generalization ability is to ﬁnd general principles\n",
            "duringtheDNN’straining. Empirically,Arpitetal.(2017)suggestedthatDNNsmaylearnsimple\n",
            "patternsﬁrstinrealdata,beforememorizing. Xuetal.(2018)foundasimilarphenomenonempiri-\n",
            "cally,whichisreferredtoFrequencyPrinciple(F-Principle),thatis,foralow-frequencydominantfunction,DNNswithcommonsettingsﬁrstquicklycapturethedominantlow-frequencycompo-nentswhilekeepingtheamplitudesofhigh-frequencycomponentssmall,andthenrelativelyslowlycapturesthosehigh-frequencycomponents.F-PrinciplecanexplainhowthetrainingcanleadDNNstoagoodgeneralizationempiricallybyshowingthattheDNNpreferstoﬁtthetargetfunctionbyalow-complexityfunctionXuetal.(2018).Rahamanetal.(2018)foundasimilarresultthat“LowerFrequenciesareLearnedFirst”.Tounderstandthisphenomenon,Rahamanetal.(2018)estimatedaninequalitythattheamplitudeofeachfrequencycomponentoftheDNNoutputiscontrolledbythespectralnormofDNNweightstheoretically.Rahamanetal.(2018)thenshowthatthespectralnorm2increasesgraduallyduringtrainingwithasmall-sizeDNNempirically.Therefore,thein-equalityimpliesthat“longertrainingallowsthenetworktorepresentmorecomplexfunctionsbyallowingittoalsoﬁthigherfrequencies”.However,foralarge-sizeDNN,thespectralnormalmostdoesnotchangeduringthetraining(SeeanexampleinFig.5inAppendix.),thatis,theboundoftheamplitudeofeachfrequencycomponentoftheDNNoutputalmostdoesnotchangeduringthetrain-ing.Then,theinequalityinRahamanetal.(2018)cannotexplainwhytheF-Principlestillholdsforalarge-sizeDNN.\n",
            "Contribution In this work, we develop a theoretical framework by Fourier analysis aiming to\n",
            "understandthetrainingprocessandthegeneralizationabilityofDNNswithsufﬁcientneuronsandhiddenlayers.Weshowthatforanyparameter,thegradientdescentmagnitudeineachfrequencycomponentofthelossfunctionisproportionaltotheproductoftwofactors:oneisadecaytermwithrespectto(w.r.t.)frequency;theotheristheamplitudeofthedifferencebetweentheDNNoutputandthetargetfunction.ThistheoreticalframeworkshowsthatDNNstrainedbygradient-basedmethodsendowlow-frequencycomponentswithhigherpriorityduringthetrainingprocess.Sincethepowerspectrumofthetanhfunctionexponentiallydecaysw.r.t.frequency,inwhichtheexponentialdecayrateisproportionaltotheinverseofweight.Wethenshowthatsmall(large)initializationwouldresultinsmall(large)amplitudeofhigh-frequencycomponents,thusleadingtheDNNoutputtoalow(high)complexityfunctionwithgood(bad)generalizationability.Therefore,withsmallinitialization,sufﬁcientlargeDNNscanﬁtanyfunction(Cybenko(1989))whilekeepinggoodgeneralization.\n",
            "We demonstratethattheanalysisinthisworkcanbequalitativelyextendedto generalDNNs. We\n",
            "exempliﬁedourtheoreticalresultsthroughDNNsﬁttingnaturalimages,1-dfunctionsandMNISTdataset(LeCun(1998)).\n",
            "The paperis established as follows. The commonsettings of DNNs in this work are presentedin\n",
            "Section2. ThetheoreticalframeworkisgiveninSection3. Wethenstudytheevolutionofthemean\n",
            "magnitudeof DNN parametersduringthe DNN trainingempiricallyin Section 4. The theoretical\n",
            "frameworkisvalidatedbyexperimentsinSection5. Theconclusionsanddiscussionsarefollowed\n",
            "inSection6.\n",
            "2 Methods\n",
            "The activation functionfor each neuronis tanh. We use DNNs of multiple hidden layers with no\n",
            "activation function for the output layer. The DNN is trained by Adam optimizer (Kingma&Ba\n",
            "(2014)).ParametersoftheAdamoptimizeraresettothedefaultvalues(Kingma&Ba(2014)).ThelossfunctionisthemeansquarederrorofthedifferencebetweentheDNNoutputandthetargetfunctioninthetrainingset.\n",
            "2InRahamanetal.(2018),“formatrix-valuedweights,theirspectralnormwascomputedbyevaluatingthe\n",
            "eigenvalueoftheeigenvectorobtainedwith10poweriterations. Forvector-valuedweights,wesimplyusethe\n",
            "L norm”.2\n",
            "2\n",
            "3 Theoretical framework\n",
            "In this section, we will develop a theoretical framework in the Fourier domain to understand the\n",
            "training process of DNN. For illustration, we ﬁrst use a DNN with one hidden layer with tanh\n",
            "functionσ(x)astheactivationfunction:\n",
            "ex−e−x\n",
            "σ(x)=tanh(x)= , x∈R.\n",
            "e−x+ex\n",
            "TheFouriertransformofσ(wx+b)withw,b∈Ris,\n",
            "π π i 1\n",
            "F[σ(wx+b)](k)= δ(k)+ exp(−ibk/w) , (1)\n",
            "r2 r2 |w| exp(πk/2w)−exp(−πk/2w)\n",
            "where\n",
            "∞\n",
            "F[σ(x)](k)= σ(x)exp(−ikx)dx.\n",
            "Z−∞\n",
            "ConsideraDNNwithonehiddenlayerwithN nodes,1-dinputxand1-doutput:\n",
            "N\n",
            "ϒ(x)= ∑a σ(w x+b ), a ,w ,b ∈R. (2)\n",
            "j j j j j j\n",
            "j=1\n",
            "Notethatwecallallw ,a andb asparameters,inwhichw anda areweights,andb isabias\n",
            "j j j j j j\n",
            "term. When|πk/w |islarge,withoutlossofgenerality,weassumeπk/w ≫0andw >0,\n",
            "j j j\n",
            "N π i\n",
            "F[ϒ](k)≈ ∑a exp(−(ib +π/2)k/w ) . (3)\n",
            "j(cid:20)r2 j j\n",
            "wj (cid:21)\n",
            "j=1\n",
            "WedeﬁnethedifferencebetweenDNNoutputandthetargetfunction f(x)ateachkas\n",
            "D(k),F[ϒ](k)−F[f](k).\n",
            "WriteD(k)as\n",
            "D(k)=A(k)eiθ(k), (4)\n",
            "whereA(k)andθ(k)∈[−π,π],indicatetheamplitudeandphaseofD(k),respectively. Thelossat\n",
            "frequencyk is L(k)= 1|D(k)|2, where |·| denotesthe normof a complexnumber. The totalloss\n",
            "2\n",
            "functionisdeﬁnedas:\n",
            "L=∑L(k).k\n",
            "NotethataccordingtotheParseval’stheorem3,thislossfunctionintheFourierdomainisequaltothecommonlyusedlossofmeansquarederror,thatis,\n",
            "1\n",
            "L= ∑(ϒ(x)−f(x))2, (5)\n",
            "2\n",
            "x\n",
            "At frequencyk, the amplitudeof the gradientwith respectto each parametercan be obtained(see\n",
            "Appendixfordetails),\n",
            "∂L(k) C0\n",
            "=i(C −1) A(k)exp(−πk/2w ) (6)\n",
            "1 j\n",
            "∂a w\n",
            "j j\n",
            "∂L(k) aj\n",
            "=C C A(k)exp(−πk/2w ) (7)\n",
            "0 22w3j j\n",
            "∂wj\n",
            "∂L(k) ajk\n",
            "=(C −1)C A(k)exp(−πk/2w ), (8)\n",
            "∂b 1 0 w2 j\n",
            "j j\n",
            "where\n",
            "π\n",
            "ei[θ(k)+bjk/wj]\n",
            "C = (9)\n",
            "0 r2\n",
            "3Without loss of generality, the constant factor that is related to the deﬁnition of corresponding Fourier\n",
            "Transformisignoredhere.\n",
            "3\n",
            "C =exp(−2i(b k/w +θ(k))), (10)\n",
            "1 j j\n",
            "C =[C (i(πk−2w )−2b k)+(−i(πk−2w )−2b k)], (11)\n",
            "2 1 j j j j\n",
            "Thedescentamountatanydirection,say,withrespecttoparameterΘ ,isjl\n",
            "∂L ∂L(k)\n",
            "=∑ . (12)\n",
            "∂Θ ∂Θ\n",
            "jl l jl\n",
            "TheabsolutecontributionfromfrequencyktothistotalamountatΘ is\n",
            "jl\n",
            "∂L(k)\n",
            "=A(k)exp(−|πk/2w |)G (Θ ,k), (13)\n",
            "j jl j\n",
            "(cid:12)∂Θ (cid:12)\n",
            "(cid:12) jl (cid:12)\n",
            "(cid:12) (cid:12)\n",
            "whereΘ ,{w ,b ,a },Θ(cid:12) ∈Θ ,(cid:12)G (Θ ,k)isafunctionwithrespecttoΘ andk,whichcanbe\n",
            "j j j j jl j jl j j\n",
            "foundinoneofEqs. (6,7,8).\n",
            "When the component at frequency k does not converge yet, exp(−|πk/2w |) would dominate\n",
            "j\n",
            "G (Θ ,k)forasmallw .Therefore,thebehaviorofEq.(13)isdominatedbyA(k)exp(−|πk/2w |).\n",
            "jl j j j\n",
            "Thisdominanttermalsoindicatesthatweightsaremuchmoreimportantthanbiasterms,whichwillbeveriﬁedbyMNISTdatasetlater.\n",
            "To examine the convergencebehavior of different frequency components during the training, we\n",
            "computetherelativedifferenceoftheDNNoutputand f(x)inthefrequencydomainateachrecord-\n",
            "ingstep,thatis,\n",
            "|F[f](k)−F[ϒ](k)|\n",
            "∆ (k)= . (14)\n",
            "F\n",
            "|F[f](k)|\n",
            "Throughtheaboveanalysisframework,wehavethefollowingtheorems(TheproofscanbefoundatAppendix.)\n",
            "Theorem 1. Consider a DNN with one hidden layer using tanh function σ(x) as the activation\n",
            "function. For any frequencies k and k such that k > k >0 and there exist c ,c , such that\n",
            "1 2 2 1 1 2\n",
            "A(k )>c >0,A(k )<c <∞,wehave\n",
            "1 1 2 2\n",
            "µ w : ∂L(k1) > ∂L(k2) forall j,l ∩B\n",
            "j ∂Θjl ∂Θjl δ(cid:17)\n",
            "lim (cid:16)n (cid:12) (cid:12) (cid:12) (cid:12) o =1, (15)\n",
            "(cid:12)(cid:12) (cid:12)(cid:12) (cid:12)(cid:12) µ(Bδ(cid:12)(cid:12))\n",
            "δ→0\n",
            "whereB isaballwithradiusδcenteredattheoriginandµ(·)istheLebesguemeasureofaset.δ\n",
            "Thm 1 shows that for any two non-converged frequencies, almost all sufﬁciently small weights\n",
            "satisfythatalower-frequencycomponenthasahigherpriorityduringthegradient-basedtraining.\n",
            "Theorem 2. Consider a DNN with one hidden layer with tanh function σ(x) as the activation\n",
            "function.Foranyfrequenciesk andk suchthatk >k >0,weconsidernon-degeneratesituation,\n",
            "1 2 2 1\n",
            "that is, |F[f](k )|>0, and there exist positive constantsC , ξ, ξ, andξ, such that A(k )<C ,\n",
            "1 a 1 2 2 a\n",
            "|1−C (k )|>ξ,andξ >|C (k )|>ξ. ∀ε>0. Then,foranyε>0,thereexistsM>0,forany\n",
            "1 1 2 2 1 1\n",
            "w ∈[−M,M]\\{0}suchthatthereexistsΘ ∈{w ,b ,a }satisfying\n",
            "j jl j j j\n",
            "∂L(k ) ∂L(k )\n",
            "1 2\n",
            "= , (16)\n",
            "(cid:12) ∂Θ (cid:12) (cid:12) ∂Θ (cid:12)\n",
            "(cid:12) jl (cid:12) (cid:12) jl (cid:12)\n",
            "(cid:12) (cid:12) (cid:12) (cid:12)\n",
            "wehave∆ (k )<ε. (cid:12) (cid:12) (cid:12) (cid:12)\n",
            "F 1\n",
            "The case that the gradient amplitude of the low-frequency component equals to that of the high-\n",
            "frequency component indicates that low frequency does not dominate. Thm 2 implies that when\n",
            "the deviation of the high-frequency component (A(k )) is bounded, we can always ﬁnd small\n",
            "2\n",
            "weights such that the relative error of the low-frequency component stays very small when the\n",
            "high-frequency component has the similar priority as the low-frequency component. In another\n",
            "word,whentheDNNstartstoﬁtthehigh-frequencycomponent,thelow-frequencyonestaysattheconvergedstate.\n",
            "Theorem 3. Consider a DNN with one hidden layer with tanh function σ(x) as the activation\n",
            "function.Foranyfrequenciesk andk suchthatk >k >0,weconsidernon-degeneratesituation,\n",
            "1 2 2 1\n",
            "thatis, |F[f](k )|>0, andthereexist positiveconstantsξ,ξ, andξ, suchthat|1−C (k )|>ξ\n",
            "1 1 2 1 1\n",
            "4\n",
            "htgnerts htgnerts htgnerts 0.8\n",
            "0.08 weightbias\n",
            "0.06 0.2 0.6\n",
            "sba sba sba\n",
            "fo 0.04 fo fo 0.4\n",
            "naem naem 0.1 naem\n",
            "0.02 0.2\n",
            "0 100 200 300 0 200 400 600 0 250 500 750 1000\n",
            "Epoch Epoch Epoch\n",
            "(a) std:0.06 (b) std:0.2 (c) std:0.6\n",
            "Figure 1: Magnitudeof DNN parametersduringﬁtting MNIST dataset. DNN parametersare ini-\n",
            "tialized by Gaussian distribution with mean 0 and standard deviation 0.06, 0.2, 0.6 for (a, b, c),\n",
            "respectively. Solidslinesshowthemeanmagnitudeoftheabsoluteweights(red)andtheabsolute\n",
            "bias(green)ateachtrainingepoch. Thedashedlinesarethemean±stdforthecorrespondingcolor.\n",
            "Notethatthegreenandtheredlinesalmostoverlap. WeuseatanhDNNwithwidth: 800-400-200-\n",
            "100.Thelearningrateis10−5withbatchsize400.\n",
            "andξ >|C (k )|>ξ,foranyB>0,thereexistsM>0,foranyA(k )>M,suchthatthereexists\n",
            "2 2 1 1 2\n",
            "Θ ∈{w ,b ,a }satisfying\n",
            "jl j j j\n",
            "∂L(k ) ∂L(k )\n",
            "1 2\n",
            "= 6=0, (17)\n",
            "(cid:12) ∂Θ (cid:12) (cid:12) ∂Θ (cid:12)\n",
            "(cid:12) jl (cid:12) (cid:12) jl (cid:12)\n",
            "(cid:12) (cid:12) (cid:12) (cid:12)\n",
            "wehave∆ (k )>B. (cid:12) (cid:12) (cid:12) (cid:12)\n",
            "F 1\n",
            "Thm 3 implies that for a large A(k ), when the low-frequency componentdoes not converge yet,\n",
            "2\n",
            "it alreadycan be signiﬁcantlyaffectedby the high-frequencycomponent. A large A(k ) can exist\n",
            "2\n",
            "when the targetfunctionis high-frequencydominate, thatis, a higher-frequencycomponenthas a\n",
            "largeramplitude(SeeanexampleinAppendixE).\n",
            "Next,wedemonstratethattheanalysisofEq. (13)canbequalitativelyextendedtogeneralDNNs.\n",
            "A(k) in Eq. (13) comes from the square operation in the loss function, thus, is irrelevant with\n",
            "DNN structure. exp(−|πk/2w |) in Eq. (13) comesfrom the exponentialdecay of the activation\n",
            "j\n",
            "function in the Fourier domain. The analysis of exp(−|πk/2w |) is insensitive to the following\n",
            "j\n",
            "factors. i) Activationfunction. The powerspectrumof mostactivationfunctionsdecreasesas the\n",
            "frequencyincreases;ii)Neuronnumber. ThesummationinEq. (2)doesnotaffecttheexponential\n",
            "decay;iii)Multiplehiddenlayers.Iftherearemultiplehiddenlayers,thecompositionofcontinuousactivationfunctionsisstillacontinuousfunction.Thepowerspectrumofthecontinuousfunctionstilldecaysasthefrequencyincreases;iv)High-dimensionalinput.WeneedtoconsiderthevectorformofkinEq.(1);v)High-dimensionaloutput.Thetotallossfunctionisthesummationofthelossofeachoutputnode.Therefore,theanalysisofA(k)exp(−|πk/2w|)ofasinglehiddenlayerjqualitativelyappliestodifferentactivationfunctions,neuronnumbers,multiplehiddenlayers,andhigh-dimensionalfunctions.\n",
            "4 The magnitude ofDNN parameters during training\n",
            "Since the magnitude of DNN parametersis importantto the analysis of the gradients, such as wj\n",
            "inEq. (13),westudytheevolutionofthemagnitudeofDNNparametersduringtraining. Through\n",
            "trainingDNNsbyMNISTdataset,empirically,weshowthatforanetworkwithsufﬁcientneuronsandlayers,themeanmagnitudeoftheabsolutevaluesofDNNparametersonlychangesslightlyduringthetraining.Forexample,wetrainaDNNbyMNISTdatasetwithdifferentinitialization.InFig.1,DNNparametersareinitializedbyGaussiandistributionwithmean0andstandarddeviation0.06,0.2,0.6for(a,b,c),respectively.Wecomputethemeanmagnitudeofabsoluteweightsandbiasterms.AsshowninFig.1,themeanmagnitudeoftheabsolutevalueofDNNparametersonlychangesslightlyduringthetraining.Thus,empirically,theinitializationalmostdeterminesthemagnitudeofDNNparameters.NotethatthemagnitudeofDNNparameterscanhavesigniﬁcantchangeduringtrainingwhenthenetworksizeissmall(WehavemorediscussioninDiscussion).\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def showPaperSummary(paperContent):\n",
        "    tldr_tag = \"\\n tl;dr:\"\n",
        "    openai.organization = 'org-m85c2ovFNrtbR71nWHBPcxm8'\n",
        "    openai.api_key = \"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\"\n",
        "    engine_list = openai.Engine.list() # calling the engines available from the openai api \n",
        "    \n",
        "    for page in paperContent:\n",
        "        text = page.extract_text() + tldr_tag\n",
        "        response = openai.Completion.create(engine=\"davinci\",prompt=text,temperature=0.3,\n",
        "            max_tokens=140,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0,\n",
        "            stop=[\"\\n\"]\n",
        "        )\n",
        "        print(response[\"choices\"][0][\"text\"])\n",
        "\n",
        "paperContent = pdfplumber.open(paperFilePath).pages\n",
        "showPaperSummary(paperContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-oIClg0eQUD",
        "outputId": "38b9bb88-6de9-4be3-fc5f-205e85f90d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " DNNs are often trained with stochastic gradient descent, which is a gradient descent method that is not guaranteed to converge to a local minimum. Instead, it is guaranteed to converge to a local minimum of the gradient. This means that the gradient descent method is not guaranteed to converge to a global minimum. The paper shows that this is not a problem, because the local minimum of the gradient is often a global minimum.\n",
            " the DNNs are trained to ﬁt the data, and the data is ﬁt by the DNNs.\n",
            " The gradient of the loss function with respect to each parameter is the product of the\n",
            "\n",
            "DNNparametersareinsensitivetoinitialization.\n",
            "\n",
            "\n",
            " Initialization of weights is more important than initialization of biases.\n",
            "\n",
            "\n",
            " The spectral norm of the weights is a good proxy for the convergence of the optimization.\n",
            " Theorem 1 of the paper says that if you have a DNN with one hidden layer using tanh as the activation function, and you have two frequencies k and k such that k > k >0 and there exist c ,c , such that 1 2 2 1 1 2 A(k )>c >0,A(k )<c <∞, then you can get a lower bound on the difference between the derivatives of the loss function at those two frequencies.\n",
            "\n",
            "\n",
            " We use a DNN to learn the target function f(x). We use a DNN to learn the target function f(x). We use a DNN to learn the target function f(x). We use a DNN to learn the target function f(x). We use a DNN to learn the target function f(x). We use a DNN to learn the target function f(x). We use a DNN to learn the target function f(x). We use a DNN to learn the target function f(x). We use a DNN to learn the target function f(x). We use a DNN to learn the target function f(x).\n",
            " the problem of overﬁtting is not a problem of the size of the model, but rather a problem of the size of the training set.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}